{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the annotation file\n",
    "with open(r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\test.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter out negative annotations\n",
    "data['annotations'] = [ann for ann in data['annotations'] if ann['category_id'] != 2]  # Assuming '2' is the id for negative\n",
    "\n",
    "# Remove the negative class from categories\n",
    "data['categories'] = [cat for cat in data['categories'] if cat['name'] != 'negative']\n",
    "\n",
    "# Save the updated annotations\n",
    "with open(r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\test.json\", \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Classes: ['tumor', 'positive']\n",
      "Number of images: 518\n",
      "Annotations for the first image: []\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Path to your COCO annotation file\n",
    "annotation_file = r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\train.json\"\n",
    "\n",
    "# Load the annotation file\n",
    "coco = COCO(annotation_file)\n",
    "\n",
    "# Get all the categories (classes)\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "class_names = [cat['name'] for cat in categories]\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Get information about the images and annotations\n",
    "image_ids = coco.getImgIds()\n",
    "images = coco.loadImgs(image_ids)\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "\n",
    "# Get annotations for a specific image\n",
    "img_id = image_ids[0]  # Select the first image as an example\n",
    "annotation_ids = coco.getAnnIds(imgIds=[img_id])\n",
    "annotations = coco.loadAnns(annotation_ids)\n",
    "\n",
    "print(\"Annotations for the first image:\", annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "import detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\"mri_train\", {}, r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\train.json\",  r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\train\")\n",
    "register_coco_instances(\"mri_test\", {}, r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\test.json\",  r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Unregister existing datasets if they are already registered\n",
    "for d in [\"mri_train\", \"mri_test\"]:\n",
    "    if d in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(d)\n",
    "    if d in MetadataCatalog.list():\n",
    "        MetadataCatalog.remove(d)\n",
    "\n",
    "# Now re-register\n",
    "register_coco_instances(\"mri_train\", {}, r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\train.json\",  r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\train\")\n",
    "register_coco_instances(\"mri_test\", {}, r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\test.json\",  r\"C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/30 13:40:09 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/30 13:40:09 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[10/30 13:40:09 d2.data.datasets.coco]: \u001b[0mLoaded 518 images in COCO format from C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\train.json\n",
      "\u001b[32m[10/30 13:40:09 d2.data.build]: \u001b[0mRemoved 220 images with no usable annotations. 298 images left.\n",
      "\u001b[32m[10/30 13:40:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[10/30 13:40:09 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[10/30 13:40:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/30 13:40:09 d2.data.common]: \u001b[0mSerializing 298 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/30 13:40:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n",
      "\u001b[32m[10/30 13:40:09 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=2\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/30 13:40:09 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[10/30 13:40:09 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (3, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (3,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/30 13:40:10 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[10/30 13:46:28 d2.utils.events]: \u001b[0m eta: 2:26:10  iter: 19  total_loss: 1.408  loss_cls: 1.004  loss_box_reg: 0.349  loss_rpn_cls: 0.02933  loss_rpn_loc: 0.004153    time: 19.2218  last_time: 34.0262  data_time: 0.5616  last_data_time: 0.0074   lr: 3.8962e-05  max_mem: 2174M\n",
      "\u001b[32m[10/30 13:52:26 d2.utils.events]: \u001b[0m eta: 2:15:46  iter: 39  total_loss: 1.009  loss_cls: 0.4976  loss_box_reg: 0.4114  loss_rpn_cls: 0.03844  loss_rpn_loc: 0.005148    time: 18.5349  last_time: 17.8460  data_time: 0.0056  last_data_time: 0.0069   lr: 7.8922e-05  max_mem: 2174M\n",
      "\u001b[32m[10/30 13:55:44 d2.utils.events]: \u001b[0m eta: 1:45:58  iter: 59  total_loss: 0.9422  loss_cls: 0.388  loss_box_reg: 0.5082  loss_rpn_cls: 0.02257  loss_rpn_loc: 0.005791    time: 15.5565  last_time: 2.5164  data_time: 0.0050  last_data_time: 0.0034   lr: 0.00011888  max_mem: 2174M\n",
      "\u001b[32m[10/30 13:57:48 d2.utils.events]: \u001b[0m eta: 1:06:27  iter: 79  total_loss: 0.9506  loss_cls: 0.3244  loss_box_reg: 0.6127  loss_rpn_cls: 0.006592  loss_rpn_loc: 0.004725    time: 13.1580  last_time: 2.6269  data_time: 0.0045  last_data_time: 0.0041   lr: 0.00015884  max_mem: 2174M\n",
      "\u001b[32m[10/30 14:00:52 d2.utils.events]: \u001b[0m eta: 1:02:47  iter: 99  total_loss: 0.8259  loss_cls: 0.2488  loss_box_reg: 0.5499  loss_rpn_cls: 0.003401  loss_rpn_loc: 0.003563    time: 12.3500  last_time: 2.5630  data_time: 0.0047  last_data_time: 0.0037   lr: 0.0001988  max_mem: 2174M\n",
      "\u001b[32m[10/30 14:01:18 d2.utils.events]: \u001b[0m eta: 0:41:15  iter: 119  total_loss: 0.7625  loss_cls: 0.2237  loss_box_reg: 0.5298  loss_rpn_cls: 0.008275  loss_rpn_loc: 0.005769    time: 10.4732  last_time: 0.5535  data_time: 0.0045  last_data_time: 0.0038   lr: 0.00023876  max_mem: 2175M\n",
      "\u001b[32m[10/30 14:01:28 d2.utils.events]: \u001b[0m eta: 0:34:39  iter: 139  total_loss: 0.6395  loss_cls: 0.1329  loss_box_reg: 0.4956  loss_rpn_cls: 0.00158  loss_rpn_loc: 0.003934    time: 9.0300  last_time: 0.4585  data_time: 0.0048  last_data_time: 0.0035   lr: 0.00027872  max_mem: 2175M\n",
      "\u001b[32m[10/30 14:01:38 d2.utils.events]: \u001b[0m eta: 0:19:19  iter: 159  total_loss: 0.4939  loss_cls: 0.1032  loss_box_reg: 0.3847  loss_rpn_cls: 0.001443  loss_rpn_loc: 0.004223    time: 7.9513  last_time: 0.5152  data_time: 0.0046  last_data_time: 0.0037   lr: 0.00031868  max_mem: 2175M\n",
      "\u001b[32m[10/30 14:01:49 d2.utils.events]: \u001b[0m eta: 0:13:43  iter: 179  total_loss: 0.417  loss_cls: 0.09863  loss_box_reg: 0.3048  loss_rpn_cls: 0.0006794  loss_rpn_loc: 0.004083    time: 7.1159  last_time: 0.5556  data_time: 0.0049  last_data_time: 0.0048   lr: 0.00035864  max_mem: 2175M\n",
      "\u001b[32m[10/30 14:01:59 d2.utils.events]: \u001b[0m eta: 0:05:22  iter: 199  total_loss: 0.4402  loss_cls: 0.09166  loss_box_reg: 0.3231  loss_rpn_cls: 0.002212  loss_rpn_loc: 0.003667    time: 6.4493  last_time: 0.5206  data_time: 0.0048  last_data_time: 0.0056   lr: 0.0003986  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:02:09 d2.utils.events]: \u001b[0m eta: 0:02:37  iter: 219  total_loss: 0.3426  loss_cls: 0.06657  loss_box_reg: 0.2609  loss_rpn_cls: 0.0008463  loss_rpn_loc: 0.0028    time: 5.9041  last_time: 0.5178  data_time: 0.0046  last_data_time: 0.0040   lr: 0.00043856  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:02:20 d2.utils.events]: \u001b[0m eta: 0:02:24  iter: 239  total_loss: 0.2753  loss_cls: 0.05389  loss_box_reg: 0.2167  loss_rpn_cls: 0.0001591  loss_rpn_loc: 0.002138    time: 5.4518  last_time: 0.4674  data_time: 0.0046  last_data_time: 0.0042   lr: 0.00047852  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:02:30 d2.utils.events]: \u001b[0m eta: 0:02:12  iter: 259  total_loss: 0.355  loss_cls: 0.06688  loss_box_reg: 0.2915  loss_rpn_cls: 0.0001502  loss_rpn_loc: 0.002832    time: 5.0681  last_time: 0.5120  data_time: 0.0046  last_data_time: 0.0050   lr: 0.00051848  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:02:40 d2.utils.events]: \u001b[0m eta: 0:02:01  iter: 279  total_loss: 0.3583  loss_cls: 0.06523  loss_box_reg: 0.2684  loss_rpn_cls: 9.612e-05  loss_rpn_loc: 0.003296    time: 4.7409  last_time: 0.5567  data_time: 0.0047  last_data_time: 0.0059   lr: 0.00055844  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:02:51 d2.utils.events]: \u001b[0m eta: 0:01:50  iter: 299  total_loss: 0.3707  loss_cls: 0.06866  loss_box_reg: 0.284  loss_rpn_cls: 0.0002226  loss_rpn_loc: 0.002181    time: 4.4577  last_time: 0.5202  data_time: 0.0045  last_data_time: 0.0035   lr: 0.0005984  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:03:01 d2.utils.events]: \u001b[0m eta: 0:01:38  iter: 319  total_loss: 0.2813  loss_cls: 0.0519  loss_box_reg: 0.2232  loss_rpn_cls: 0.0001104  loss_rpn_loc: 0.002984    time: 4.2094  last_time: 0.5174  data_time: 0.0046  last_data_time: 0.0040   lr: 0.00063836  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:03:11 d2.utils.events]: \u001b[0m eta: 0:01:27  iter: 339  total_loss: 0.3345  loss_cls: 0.05269  loss_box_reg: 0.266  loss_rpn_cls: 0.0002481  loss_rpn_loc: 0.002605    time: 3.9909  last_time: 0.5061  data_time: 0.0070  last_data_time: 0.0041   lr: 0.00067832  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:03:20 d2.utils.events]: \u001b[0m eta: 0:01:16  iter: 359  total_loss: 0.3066  loss_cls: 0.05523  loss_box_reg: 0.2235  loss_rpn_cls: 0.0007156  loss_rpn_loc: 0.002646    time: 3.7935  last_time: 0.5050  data_time: 0.0038  last_data_time: 0.0020   lr: 0.00071828  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:03:30 d2.utils.events]: \u001b[0m eta: 0:01:04  iter: 379  total_loss: 0.2561  loss_cls: 0.04116  loss_box_reg: 0.2056  loss_rpn_cls: 4.503e-05  loss_rpn_loc: 0.002273    time: 3.6187  last_time: 0.4982  data_time: 0.0043  last_data_time: 0.0051   lr: 0.00075824  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:03:43 d2.utils.events]: \u001b[0m eta: 0:00:53  iter: 399  total_loss: 0.3103  loss_cls: 0.05382  loss_box_reg: 0.2422  loss_rpn_cls: 0.000171  loss_rpn_loc: 0.002317    time: 3.4683  last_time: 1.7655  data_time: 0.0047  last_data_time: 0.0045   lr: 0.0007982  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:04:18 d2.utils.events]: \u001b[0m eta: 0:00:43  iter: 419  total_loss: 0.269  loss_cls: 0.05193  loss_box_reg: 0.2091  loss_rpn_cls: 0.000227  loss_rpn_loc: 0.00259    time: 3.3873  last_time: 1.8488  data_time: 0.0047  last_data_time: 0.0048   lr: 0.00083816  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:04:54 d2.utils.events]: \u001b[0m eta: 0:00:32  iter: 439  total_loss: 0.3578  loss_cls: 0.05059  loss_box_reg: 0.3088  loss_rpn_cls: 0.0001889  loss_rpn_loc: 0.003162    time: 3.3135  last_time: 1.7556  data_time: 0.0046  last_data_time: 0.0042   lr: 0.00087812  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:05:29 d2.utils.events]: \u001b[0m eta: 0:00:22  iter: 459  total_loss: 0.259  loss_cls: 0.04313  loss_box_reg: 0.2137  loss_rpn_cls: 4.709e-05  loss_rpn_loc: 0.002398    time: 3.2463  last_time: 2.0215  data_time: 0.0047  last_data_time: 0.0041   lr: 0.00091808  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:06:05 d2.utils.events]: \u001b[0m eta: 0:00:11  iter: 479  total_loss: 0.2581  loss_cls: 0.04417  loss_box_reg: 0.2084  loss_rpn_cls: 0.0002353  loss_rpn_loc: 0.002445    time: 3.1863  last_time: 1.6049  data_time: 0.0039  last_data_time: 0.0066   lr: 0.00095804  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:06:44 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 499  total_loss: 0.3347  loss_cls: 0.05527  loss_box_reg: 0.2768  loss_rpn_cls: 0.0001387  loss_rpn_loc: 0.002337    time: 3.1320  last_time: 1.8541  data_time: 0.0046  last_data_time: 0.0040   lr: 0.000998  max_mem: 2176M\n",
      "\u001b[32m[10/30 14:06:44 d2.engine.hooks]: \u001b[0mOverall training speed: 498 iterations in 0:25:59 (3.1321 s / it)\n",
      "\u001b[32m[10/30 14:06:44 d2.engine.hooks]: \u001b[0mTotal training time: 0:26:01 (0:00:02 on hooks)\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/30 14:06:44 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[10/30 14:06:44 d2.data.datasets.coco]: \u001b[0mLoaded 36 images in COCO format from C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\test.json\n",
      "\u001b[32m[10/30 14:06:44 d2.data.build]: \u001b[0mDistribution of instances among all 2 categories:\n",
      "\u001b[36m|  category  | #instances   |  category  | #instances   |\n",
      "|:----------:|:-------------|:----------:|:-------------|\n",
      "|   tumor    | 0            |  positive  | 7            |\n",
      "|            |              |            |              |\n",
      "|   total    | 7            |            |              |\u001b[0m\n",
      "\u001b[32m[10/30 14:06:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[10/30 14:06:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/30 14:06:44 d2.data.common]: \u001b[0mSerializing 36 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/30 14:06:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/30 14:06:44 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/30 14:06:44 d2.evaluation.coco_evaluation]: \u001b[0mCOCO Evaluator instantiated using config, this is deprecated behavior. Please pass in explicit arguments instead.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/30 14:06:44 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[10/30 14:06:44 d2.data.datasets.coco]: \u001b[0mLoaded 36 images in COCO format from C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\test.json\n",
      "\u001b[32m[10/30 14:06:44 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[10/30 14:06:44 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[10/30 14:06:44 d2.data.common]: \u001b[0mSerializing 36 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[10/30 14:06:44 d2.data.common]: \u001b[0mSerialized dataset takes 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "# Configuring the Fast R-CNN model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(detectron2.model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"mri_train\",)\n",
    "cfg.DATASETS.TEST = (\"mri_test\",)  # For evaluation\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = detectron2.model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.001  # Learning rate\n",
    "cfg.SOLVER.MAX_ITER = 500  # Adjust based on your needs\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # Tumor, positive\n",
    "\n",
    "# Setup output directory\n",
    "cfg.OUTPUT_DIR = \"./output_mri\"\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=False)\n",
    "\n",
    "# Trainer setup\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "evaluator = COCOEvaluator(\"mri_test\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "test_loader = build_detection_test_loader(cfg, \"mri_test\")  # No need to call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/30 14:09:58 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./output_mri\\model_final.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\anaconda3\\envs\\detectron_env\\lib\\site-packages\\fvcore\\common\\checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[10/30 14:09:58 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[10/30 14:09:58 d2.data.datasets.coco]: \u001b[0mLoaded 36 images in COCO format from C:\\Users\\moham\\Desktop\\computer vision\\Axial-Dataset.v4-axial.coco\\annotations\\test.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# Load the trained model\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set a threshold for making predictions\n",
    "predictor = detectron2.engine.DefaultPredictor(cfg)\n",
    "\n",
    "# Load the test dataset\n",
    "dataset_dicts = DatasetCatalog.get(\"mri_test\")\n",
    "\n",
    "# Visualize predictions on a few random test images\n",
    "for d in random.sample(dataset_dicts, 3):  # Display predictions on 3 random images\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(img)  # Run inference\n",
    "\n",
    "    # Visualize the predictions\n",
    "    v = Visualizer(img[:, :, ::-1],\n",
    "                   metadata=MetadataCatalog.get(\"mri_test\"), \n",
    "                   scale=0.8, \n",
    "                   instance_mode=ColorMode.IMAGE_BW  # Remove the color of the unsegmented parts\n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    \n",
    "    # Display the image with predictions\n",
    "    cv2.imshow(\"Predicted Image\", v.get_image()[:, :, ::-1])\n",
    "    cv2.waitKey(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
